{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630f355c",
   "metadata": {},
   "source": [
    "# An Introduction to JAX\n",
    "\n",
    "#### John Stachurski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ada693",
   "metadata": {},
   "source": [
    "[JAX](https://github.com/google/jax) is scientific library within the Python ecosystem that provides data types, functions and a compiler for fast linear algebra operations and automatic differentiation.\n",
    "\n",
    "Loosely speaking, JAX is like NumPy with the addition of\n",
    "\n",
    "* automatic differentiation\n",
    "* automated GPU/TPU support\n",
    "* a just-in-time compiler\n",
    "\n",
    "JAX is often used for machine learning and AI, since it can scale to big data operations on GPUs and automatically differentiate loss functions for gradient decent.\n",
    "\n",
    "However, JAX is sufficiently low-level that it can be used for many purposes.\n",
    "\n",
    "Here is a short history of JAX:\n",
    "\n",
    "* 2015: Google open-sources part of its AI infrastructure called TensorFlow.\n",
    "* 2016: The popularity of TensorFlow grows rapidly.\n",
    "* 2017: Facebook open-sources PyTorch beta, an alternative AI framework (developer-friendly, more Pythonic)\n",
    "* 2018: Facebook launches a full production-ready version of PyTorch.\n",
    "* 2019: PyTorch surges in popularity (adopted by Uber, Airbnb, Tesla, etc.)\n",
    "* 2020: Google launches JAX as an open-source framework.\n",
    "* 2021: Google starts to shift away from TPUs to Nvidia GPUs, extends JAX capabilities.\n",
    "* 2022: Uptake of Google JAX accelerates rapidly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0ab12",
   "metadata": {},
   "source": [
    "We begin this notebook with some standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from numba import jit, njit, float64, vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d12e7c",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "JAX can be installed with or without GPU support.\n",
    "\n",
    "* Follow [the install guide](https://github.com/google/jax)\n",
    "\n",
    "Note that JAX is pre-installed with GPU support on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "(Colab Pro offers better GPUs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f53e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8b804",
   "metadata": {},
   "source": [
    "## JAX as a NumPy Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bf3b4",
   "metadata": {},
   "source": [
    "One way to use JAX is as a plug-in NumPy replacement.  Let's look at the similarities and differences.\n",
    "\n",
    "### Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396c47",
   "metadata": {},
   "source": [
    "The following import is standard, replacing `import numpy as np`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83698fd7",
   "metadata": {},
   "source": [
    "Now we can use `jnp` in place of `np` for the usual array operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.asarray((1.0, 3.2, -1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c671b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.dot(a, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc7427",
   "metadata": {},
   "source": [
    "However, the array object `a` is not a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834471e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37420923",
   "metadata": {},
   "source": [
    "Even scalar-valued maps on arrays return JAX arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79771a05",
   "metadata": {},
   "source": [
    "JAX arrays are allocated on the `device`.\n",
    "\n",
    "Here `device` refers to the hardware accelerator (GPU or TPU), although JAX falls back to the CPU if no accelerator is detected.\n",
    "\n",
    "(In the terminology of GPUs, the \"host\" is the machine that launches GPU operations, while the \"device\" is the GPU itself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868e344",
   "metadata": {},
   "source": [
    "Operations on higher dimensional arrays is also similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077492b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones((2, 2))\n",
    "B = jnp.identity(2)\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e397737",
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.solve(B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa66b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.eigh(B)  # Computes eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de7cf",
   "metadata": {},
   "source": [
    "### Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665733c",
   "metadata": {},
   "source": [
    "One difference between NumPy and JAX is that, when running on a GPU, JAX uses 32 bit floats by default.  This is standard for GPU computing and can lead to significant speed gains with small loss of precision.\n",
    "\n",
    "However, for some calculations precision matters.  In these cases 64 bit floats can be enforced via the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f76411",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06381b1a",
   "metadata": {},
   "source": [
    "Let's check this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.ones(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fc283",
   "metadata": {},
   "source": [
    "As a NumPy replacement, a more significant difference is that arrays are treated as **immutable**.  For example, with NumPy we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6413340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.linspace(0, 1, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53758dd",
   "metadata": {},
   "source": [
    "and then mutate the data in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5a219",
   "metadata": {},
   "source": [
    "In JAX this fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.linspace(0, 1, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23537ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657f546",
   "metadata": {},
   "source": [
    "In line with immutability, JAX does not support inplace operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array((2, 1))\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.array((2, 1))\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91070df",
   "metadata": {},
   "source": [
    "The designers of JAX chose to make arrays immutable because JAX uses a functional programming style.  More on this below.  \n",
    "\n",
    "Note that, while mutation is discouraged, it is in fact possible with `at`, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.linspace(0, 1, 3)\n",
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.at[0].set(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdea30",
   "metadata": {},
   "source": [
    "We can check that the array is mutated by verifying its identity is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187e113",
   "metadata": {},
   "source": [
    "## Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd35860",
   "metadata": {},
   "source": [
    "Random numbers are also a bit different in JAX, relative to NumPy.  Typically, in JAX, the state of the random number generator needs to be controlled explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f571c",
   "metadata": {},
   "source": [
    "First we produce a key, which seeds the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f503397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e795f",
   "metadata": {},
   "source": [
    "Now we can use the key to generate some random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.normal(key, (3, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb497a40",
   "metadata": {},
   "source": [
    "If we use the same key again, we initialize at the same seed, so the random numbers are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27e7f7",
   "metadata": {},
   "source": [
    "To produce a (quasi-) independent draw, best practice is to \"split\" the existing key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e04076",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(subkey, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f99a5",
   "metadata": {},
   "source": [
    "The function below produces `k` (quasi-) independent random `n x n` matrices using this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_matrices(key, n, k):\n",
    "    matrices = []\n",
    "    for _ in range(k):\n",
    "        key, subkey = random.split(key)\n",
    "        matrices.append(random.uniform(subkey, (n, n)))\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = gen_random_matrices(key, 2, 2)\n",
    "for A in matrices:\n",
    "    print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdda9da",
   "metadata": {},
   "source": [
    "One point to remember is that JAX expects tuples to describe array shapes, even for flat arrays.  Hence, to get a one-dimensional array of normal random draws we use `(len, )` for the shape, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (5, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24e1fa",
   "metadata": {},
   "source": [
    "## JIT Compilation\n",
    "\n",
    "\n",
    "The JAX JIT compiler accelerates logic within functions by fusing linear algebra operations into a single, highly optimized kernel that the host can launch on the GPU / TPU (or CPU if no accelerator is detected)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d27a04",
   "metadata": {},
   "source": [
    "Consider the following pure Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3300058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1000):\n",
    "    return sum((k*x for k in range(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00134054",
   "metadata": {},
   "source": [
    "Let's build an array to call the function on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50_000_000\n",
    "x = jnp.ones(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14d969",
   "metadata": {},
   "source": [
    "How long does the function take to execute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff26aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46564d43",
   "metadata": {},
   "source": [
    "This code is not particularly fast.  While it is run on the GPU, since `x` is a DeviceArray, each vector `k * x` has to be instantiated before the final sum is computed.\n",
    "\n",
    "If we JIT-compile the function with JAX, then the operations are fused and no intermediate arrays are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b98aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit = jax.jit(f)   # target for JIT compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c42e2a",
   "metadata": {},
   "source": [
    "Let's run once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f21965",
   "metadata": {},
   "source": [
    "And now let's time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc865670",
   "metadata": {},
   "source": [
    "## Functional Programming\n",
    "\n",
    "From JAX's documentation:\n",
    "\n",
    "*When walking about the countryside of Italy, the people will not hesitate to tell you that JAX has \"una anima di pura programmazione funzionale\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81b402",
   "metadata": {},
   "source": [
    "In other words, JAX assumes a functional programming style.\n",
    "\n",
    "The major implication is that JAX functions should be pure:\n",
    "    \n",
    "* no dependence on global variables\n",
    "* no side effects\n",
    "\n",
    "\"A pure function will always return the same result if invoked with the same inputs.\"\n",
    "\n",
    "JAX will not usually throw errors when compiling impure functions but execution becomes unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbe5e1",
   "metadata": {},
   "source": [
    "Here's an illustration of this fact, using global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86917164",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1  # global\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return a + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e553f5",
   "metadata": {},
   "source": [
    "In the code above, the global value `a=1` is fused into the jitted function.\n",
    "\n",
    "Even if we change `a`, the output of `f` will not be affected --- as long as the same compiled version is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f321cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a85b5",
   "metadata": {},
   "source": [
    "Changing the dimension of the input triggers a fresh compilation of the function, at which time the change in the value of `a` takes effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6979d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a37630",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61404a9d",
   "metadata": {},
   "source": [
    "Moral of the story: write pure functions when using JAX!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d7f1b",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214d2db",
   "metadata": {},
   "source": [
    "JAX can use automatic differentiation to compute gradients.\n",
    "\n",
    "This can be extremely useful in optimization, root finding and other applications.\n",
    "\n",
    "Here's a very simple illustration, involving the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a05c5",
   "metadata": {},
   "source": [
    "Let's take the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b178bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime(10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb624f",
   "metadata": {},
   "source": [
    "Let's plot the function and derivative, noting that $f'(x) = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabe5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_grid = jnp.linspace(-4, 4, 200)\n",
    "ax.plot(x_grid, f(x_grid), label=\"$f$\")\n",
    "ax.plot(x_grid, jax.vmap(f_prime)(x_grid), label=\"$f'$\")\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a55e11c",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Recall that Newton's method for solving for the root of $f$ involves iterating on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17aef2e",
   "metadata": {},
   "source": [
    "$$ q(x) = x - \\frac{f(x)}{f'(x)} $$\n",
    "\n",
    "Write a function called `newton` that takes a function $f$ plus a guess $x_0$ and returns an approximate fixed point.  Your `newton` implementation should use automatic differentiation to calculate $f'$.\n",
    "\n",
    "Test your `newton` method on the function shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aeab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: jnp.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "x = jnp.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x), label='$f(x)$')\n",
    "ax.axhline(ls='--', c='k')\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.set_ylabel('$f(x)$', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83391df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3f0b1",
   "metadata": {},
   "source": [
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b23f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, x_0, tol=1e-5):\n",
    "    f_prime = jax.grad(f)\n",
    "    def q(x):\n",
    "        return x - f(x) / f_prime(x)\n",
    "\n",
    "    error = tol + 1\n",
    "    x = x_0\n",
    "    while error > tol:\n",
    "        y = q(x)\n",
    "        error = abs(x - y)\n",
    "        x = y\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbec631",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(f, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1b5d9",
   "metadata": {},
   "source": [
    "This number looks good, given the figure.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "This exercise uses parallelized gradient ascent to maximize a function.\n",
    "\n",
    "Here's the function we want to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    return jnp.cos(x[0]**2 + x[1]**2) / (1 + x[0]**2 + x[1]**2) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71fb73",
   "metadata": {},
   "source": [
    "Here's one update step of gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_grad = jax.grad(f)\n",
    "\n",
    "def update(x, f, f_grad, alpha=0.01):\n",
    "    return x + alpha * f_grad(x)\n",
    "\n",
    "x_0 = jnp.array((0.7, 0.7))\n",
    "\n",
    "update(x_0, f, f_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f435d15",
   "metadata": {},
   "source": [
    "Let's vectorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vec = jax.vmap(update, (0, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd2770",
   "metadata": {},
   "source": [
    "Let's test that this works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "n = 1000\n",
    "xs = jax.random.uniform(key, (n, 2), minval=-3.0, maxval=3.0)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vec(xs, f, f_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135cfad9",
   "metadata": {},
   "source": [
    "We have updated every 2-vector in `xs` (every row) using the update rule.\n",
    "\n",
    "The exercise is to run this in a loop and compute an approximate maximum of\n",
    "the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3875f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c0b96",
   "metadata": {},
   "source": [
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "solution below\n",
    "\n",
    "\n",
    "Here's a suitable function for the loop phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36302266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(f, f_grad, x_0, tol=1e-8, alpha=1e-2, max_iter=10_000):\n",
    "    error = tol + 1\n",
    "    x = x_0\n",
    "    i = 0\n",
    "    current_max = - jnp.inf\n",
    "    while error > tol and i < max_iter:\n",
    "        y = update_vec(x, f, f_grad)\n",
    "        new_max = jnp.max(jax.vmap(f)(x))\n",
    "        error = abs(new_max - current_max)\n",
    "        current_max = new_max\n",
    "        x = y\n",
    "        i += 1\n",
    "        \n",
    "    return current_max, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfbb91",
   "metadata": {},
   "source": [
    "Now let's call it, starting from `xs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a105987",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_ascent(f, f_grad, xs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
